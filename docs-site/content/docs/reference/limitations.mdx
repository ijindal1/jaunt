---
title: Limitations / Gotchas
description: "Current behavioral constraints in the MVP implementation."
---

Jaunt is an MVP. These are the sharp edges you should know about.

## Provider Support

Only `llm.provider = "openai"` is supported by the CLI today. The config loader accepts a `provider` field, but the CLI will reject anything other than `"openai"` with a config error. If you need a different backend, you'd need to implement the backend interface yourself and use it programmatically.

## Hardcoded `__generated__` Dir Name

Runtime forwarding for `@jaunt.magic` currently imports from `__generated__/` (hardcoded in the decorator). If you set `paths.generated_dir` to a custom name in `jaunt.toml`, `jaunt build` will write files there, but calling your `@jaunt.magic` functions at runtime will fail because the decorator still looks for `__generated__/`.

Workaround: keep `generated_dir = "__generated__"`, or import the generated module directly instead of relying on the decorator forwarding (not recommended if you want the "spec is the API" workflow).

## Prompt Overrides

The `[prompts]` config keys (`build_system`, `build_module`, `test_system`, `test_module`) are treated as **file paths** by the OpenAI backend. If you set them, those files must exist on disk and contain the full prompt text — they're not inline strings.

If you want to tweak prompts, copy the defaults from `src/jaunt/prompts/`, modify them, and version the prompt files alongside your repo.

## Dependency Context Plumbing

The dependency DAG, ordering, and staleness propagation all work correctly. However, the backend does not currently pass rich "here is the generated dependency source" context to dependents. The LLM generating a dependent spec doesn't see what its dependencies actually implemented.

In practice, this means you may need to be more explicit in docstrings (or add `prompt=` context) when a spec depends on non-trivial behavior from another generated spec. Ordering and digest-based staleness still work — it's only the prompt context that's limited.

## Auto-Generated PyPI Skills

The skills system adds extra network calls (to PyPI for README fetches) and extra OpenAI calls (to generate skill docs) during `jaunt build`. These are best-effort: failures emit a warning and the build continues without the missing skills.

This means build output can vary based on your environment, network connectivity, and what's installed in the active venv. If reproducibility matters, commit the generated `.agents/skills/` directory.

Next: [Development](/docs/development/contributing).
